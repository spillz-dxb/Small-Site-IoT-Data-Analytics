
# Objection 

Be able to pull data in from IoT devices and perform data processing and ML in R/Python or Scala using notebooks




My Objection

Install Spark
Install Zeppelin
Install PostgreSQL and timescale DB

Use Python/Scala And R in spark
When using R in Zeppelin set the you need to use %spark.r if you want spark to process the data. Using %r will only have zeppelin talk with r-base

Connect spark/scala to timescale DB in Zeppelin using scala


The end end result you can pull data in using Python play with it store it in postgress. Use scala if need be using zeppelin z.get/set methods, Call the data from PostgreSQL using R

z.get set methods in zeppelin 

https://zeppelin.apache.org/docs/0.8.0/usage/other_features/zeppelin_context.html#object-exchange



# Data Processsing 

Apache Spark


# Notebooks
Apache Zeppelin
https://zeppelin.apache.org/


# DB

Timescale DB (Psotgres)

https://www.timescale.com/


# IoT data sync

## Haystack
We need to pull data from existing IoT projects using Haystack.

https://project-haystack.org/


## Json
Need to be able to get data from by method of json


# Setup 
## Spark
https://spark.apache.org/downloads.html

## Zeppelin 

Download and start in bin folder ./zeppelin.sh




# spark-iot-ts
A platform for IOT timeseries analytics using Apache Spark

## Objective of project
A  Data backed platform for analytics of timeseries IOT data in cloud.

## Features of this project
Some of the features of this project are:
- Ease of reading IOT devices histories using metadata and time range using project-haystack format
- Helpers and utilities for time series data analytics and processing
- Temporal (preserving sorting by time) merge, joins and other operations using flint ts library
- Support for streaming, batch analytics and ad-hoc queries using Apache Spark and FiloDB (built on top of Apache Cassandra)


### Apache Spark
[Apache Spark](https://spark.apache.org/) along with [FiloDB](https://github.com/filodb/FiloDB) library and
 [Flint ts](https://github.com/twosigma/flint) library is the core of this project.

> Apache Sparkâ„¢ is a fast and general engine for large-scale data processing.

 Apache Spark can be used for batch processing, streaming, machine learning and graph analytics on data generated by iot devices.

> The ability to analyze time series data at scale is critical for the success of finance and IoT applications based on Spark


### Zeppelin Notebook
> Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more.

[Apache Zeppelin](https://zeppelin.apache.org) is used to write business logic and rules and perform other data analytics.




## Schema

### Metadata Schema

Metadata is stored in elasticsearch, hence the rules or convention regarding elasticsearch mapping or field names should be followed.
The general idea is to ETL haystack format metadata from edge devices to elasticsearch in cloud.
In general, key-value pair will be used as tag or metadata.
The markers of haystack format will be changed to marker as field name and value as 1.
The elasticsearch type should be "metadata" (as convention) and the index should be aliased to "metadata".

Example 1 (Equipment Metadata):

```
{
"_index": "metadata_equip_v1",
"_type": "metadata",
"_id": "site_boiler_1",
"_score": 1,
"_source": {
"id": "site_boiler_1",
"equip": 1,
"boiler": 1,
"hvac": 1,
"capacity": 100,
"name": "Boiler 1",
"equipRef": "Site Boiler 1",
"levelRef": "level1",
"siteRef": "site"
}
}
```
Example 2 (Points Metadata):
```
{
"_index": "metadata_v2",
"_type": "metadata",
"_id": "OAH",
"_score": 1,
"_source": {
"hisSize": "27,292",
"mod": "11-Apr-2017 Tue 01:23:30 UTC",
"tz": "Sydney",
"air": 1,
"point": 1,
"dis": "OAH",
"analytics": 1,
"regionRef": "Western Corridor",
"his": 1,
"disMacro": "$equipRef $navName",
"imported": 1,
"humidity": 1,
"navName": "OAH",
"equipRef": "Site Building Info",
"id": "Site Building Info OAH",
"hisRollup": "max",
"hisEnd": "12-Oct-2017 Thu 07:00:01 AEDT",
"hisStart": "1-Jan-2017 Sun 00:15:00 AEDT",
"levelRef": "Site Plant",
"haystackConnRef": "DEMO_CLIENT",
"hisStatus": "ok",
"hisId": "84.095",
"kind": "Number",
"siteRef": "Site",
"hisEndVal": "67.959 %",
"unit": "%",
"haystackHis": "H.DEMO_ST.DEMO_Plant_ACU~242d1~2420OAH",
"outside": 1,
"sensor": 1
}
}
```

### Histories Schema
Following are the fields for history data.

timestamp, datetime, pointName and value are the timeseries data.

siteRef and yearMonth are for partition.

siteRef and equipName generally will be used as joining keys to get different points of same equipment.

For a partition, the data will be sorted by time which is main advantage of using FiloDB for timeseries analytics.


```
 |-- pointName: string (nullable = false)
 |-- timestamp: timestamp (nullable = false)
 |-- datetime: long (nullable = false)
 |-- equipName: string (nullable = false)
 |-- siteRef: string (nullable = false)
 |-- unit: string (nullable = true)
 |-- yearMonth: string (nullable = false)
 |-- value: double (nullable = true)
```

Sample Data:
```
+--------------------+--------------------+-------------------+--------------+-------+----+---------+-----+
|           pointName|           timestamp|           datetime|     equipName|siteRef|unit|yearMonth|value|
+--------------------+--------------------+-------------------+--------------+-------+----+---------+-----+
|S.site.hayTest.Bool1|2018-02-02 00:19:...|1517530772000000000|S.site.hayTest| S.site|    |  2018-02|  0.0|
|S.site.hayTest.Bool1|2018-02-02 00:20:...|1517530803000000000|S.site.hayTest| S.site|    |  2018-02|  1.0|
|S.site.hayTest.Bool1|2018-02-02 00:20:...|1517530805000000000|S.site.hayTest| S.site|    |  2018-02|  0.0|
|S.site.hayTest.Bool1|2018-02-02 00:20:...|1517530811000000000|S.site.hayTest| S.site|    |  2018-02|  1.0|
|S.site.hayTest.Bool1|2018-02-02 00:20:...|1517530815000000000|S.site.hayTest| S.site|    |  2018-02|  0.0|
|S.site.hayTest.Bool1|2018-02-02 00:20:...|1517530824000000000|S.site.hayTest| S.site|    |  2018-02|  0.0|
+--------------------+--------------------+-------------------+--------------+-------+----+---------+-----+
```

## Examples



### Reading point using metadata and history range

```
from au.com.gegroup.ts.datetime.utils import *
boolHis = reader.metadata('his and point and id == @S.site.hayTest.Bool1', key_col="id").history(this_month()).read()
```


### Using Apache Spark and Flint Ts Library for timeseries operations

**1) Aggregating on whole history**
```
his_reader.get_df().select(func.min("timestamp").alias("min_date"), func.max("timestamp").alias("max_date")).show()
+--------------------+--------------------+
|            min_date|            max_date|
+--------------------+--------------------+
|2018-02-02 00:19:...|2018-03-25 21:50:...|
+--------------------+--------------------+
```

**2) Joining timeseries dataframes (Temporal Joins)**
```
num1His = reader.metadata('his and point and id == @S.site.hayTest.Num1', key_col="id").history(this_month()).read()
boolHis = reader.metadata('his and point and id == @S.site.hayTest.Bool1', key_col="id").history(this_month()).read()

joinedHis = num1His.leftJoin(boolHis, tolerance="1 days", key =["siteRef", "equipName"], right_alias="bool1")
```

**3) Merging timeseries dataframes (with same schema) preserving sorting by time**
```


```

**4) Time window based summarizers**
```
# Showing variance of value in past 1 day using summarizeWindows


+-------------------+--------------------+-------+-------------------+-------+--------------+--------------------+
|               time|           timestamp|  value|          pointName|siteRef|     equipName|      value_variance|
+-------------------+--------------------+-------+-------------------+-------+--------------+--------------------+
|1520563500017999872|2018-03-09 02:45:...|50.3811|S.site.hayTest.Num2| S.site|S.site.hayTest|                 NaN|
|1520564400012999936|2018-03-09 03:00:...|50.6379|S.site.hayTest.Num2| S.site|S.site.hayTest| 0.03297311999999958|
|1520565300008000000|2018-03-09 03:15:...|50.7815|S.site.hayTest.Num2| S.site|S.site.hayTest| 0.04114789333333285|
|1520566200001999872|2018-03-09 03:30:...|50.6191|S.site.hayTest.Num2| S.site|S.site.hayTest|0.027521546666666372|
|1520567100028000000|2018-03-09 03:45:...|50.0814|S.site.hayTest.Num2| S.site|S.site.hayTest| 0.07545160999999947|
|1520568000007000064|2018-03-09 04:00:...|50.9536|S.site.hayTest.Num2| S.site|S.site.hayTest| 0.09462321466666548|
+-------------------+--------------------+-------+-------------------+-------+--------------+--------------------+
```

**5) Time window based custom UDF**
```
# Calculating moving average for past 2 days



+-------------------+--------------------+-------+-------------------+-------+--------------+------------------+
|               time|           timestamp|  value|          pointName|siteRef|     equipName|     movingAverage|
+-------------------+--------------------+-------+-------------------+-------+--------------+------------------+
|1520563500017999872|2018-03-09 02:45:...|50.3811|S.site.hayTest.Num2| S.site|S.site.hayTest|           50.3811|
|1520564400012999936|2018-03-09 03:00:...|50.6379|S.site.hayTest.Num2| S.site|S.site.hayTest|           50.5095|
|1520565300008000000|2018-03-09 03:15:...|50.7815|S.site.hayTest.Num2| S.site|S.site.hayTest| 50.60016666666667|
|1520566200001999872|2018-03-09 03:30:...|50.6191|S.site.hayTest.Num2| S.site|S.site.hayTest|           50.6049|
|1520567100028000000|2018-03-09 03:45:...|50.0814|S.site.hayTest.Num2| S.site|S.site.hayTest|           50.5002|
|1520568000007000064|2018-03-09 04:00:...|50.9536|S.site.hayTest.Num2| S.site|S.site.hayTest|50.575766666666674|
|1520568900016999936|2018-03-09 04:15:...|50.4218|S.site.hayTest.Num2| S.site|S.site.hayTest| 50.55377142857144|
|1520569800011000064|2018-03-09 04:30:...| 50.213|S.site.hayTest.Num2| S.site|S.site.hayTest| 50.51117500000001|
|1520570700011000064|2018-03-09 04:45:...|50.2189|S.site.hayTest.Num2| S.site|S.site.hayTest| 50.47870000000001|
+-------------------+--------------------+-------+-------------------+-------+--------------+------------------+
```

**6) Summarizing dataframes**
```


+----+-------+--------------+--------------------+-------------------+
|time|siteRef|     equipName|           pointName|       value_stddev|
+----+-------+--------------+--------------------+-------------------+
|   0| S.site|S.site.hayTest|S.site.hayTest.Bool1|                0.0|
|   0| S.site|S.site.hayTest| S.site.hayTest.Num2|0.28202272739583134|
|   0| S.site|S.site.hayTest| S.site.hayTest.Num1|0.28505566501478885|
+----+-------+--------------+--------------------+-------------------+
```

**7) Rolling up histories (Summarizing by intervals)**
```

+-------------------+-------+--------------+--------------------+------------------+--------------------+
|               time|siteRef|     equipName|           pointName|        value_mean|           timestamp|
+-------------------+-------+--------------+--------------------+------------------+--------------------+
|1520553600000000000| S.site|S.site.hayTest|S.site.hayTest.Bool1|               0.0|2018-03-09 00:00:...|
|1520553600000000000| S.site|S.site.hayTest| S.site.hayTest.Num2|50.553811764705884|2018-03-09 00:00:...|
|1520640000000000000| S.site|S.site.hayTest| S.site.hayTest.Num2|        50.4913375|2018-03-10 00:00:...|
|1520640000000000000| S.site|S.site.hayTest|S.site.hayTest.Bool1|               0.0|2018-03-10 00:00:...|
|1520726400000000000| S.site|S.site.hayTest| S.site.hayTest.Num2|      50.509790625|2018-03-11 00:00:...|
|1520726400000000000| S.site|S.site.hayTest|S.site.hayTest.Bool1|               0.0|2018-03-11 00:00:...|
+-------------------+-------+--------------+--------------------+------------------+--------------------+

```




